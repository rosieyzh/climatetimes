{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This scraper was used to scrape articles from the Mother Jones news site.\n",
    "I used some of Rosie's code to create this scraper\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_links(result_link):\n",
    "    \"\"\"Gets all article links on a given page on a news site\"\"\"\n",
    "    links=[]\n",
    "    r = requests.get(result_link)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    links_results = soup.findAll('h3', {\"class\":\"hed\"})\n",
    "    \n",
    "    for result in links_results:\n",
    "        links.append(result.contents[1]['href'])\n",
    "\n",
    "    return links\n",
    "\n",
    "def get_text(article_link):\n",
    "    \"\"\"Takes in a link to an article and returns the formatted article as text\"\"\"\n",
    "    article=\"\"\n",
    "    r = requests.get(article_link)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    article_body = soup.find('article', {\"class\":\"entry-content\"})\n",
    "\n",
    "    for p in article_body.findAll('p'):\n",
    "        excerpt = p.text.replace(u'\\xa0', u' ')\n",
    "        article = article + \" \" + excerpt\n",
    "        \n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links = get_links(\"https://www.motherjones.com/topics/climate-change/page/1\")\n",
    "# print(get_text(links[0]))\n",
    "# links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 1\n",
      "Page: 2\n",
      "Page: 3\n",
      "Page: 4\n",
      "Page: 5\n",
      "Page: 6\n",
      "Page: 7\n",
      "Page: 8\n",
      "Page: 9\n",
      "Page: 10\n",
      "Page: 11\n",
      "Page: 12\n",
      "Page: 13\n",
      "Page: 14\n",
      "Page: 15\n",
      "Page: 16\n",
      "Page: 17\n",
      "Page: 18\n",
      "Page: 19\n",
      "Page: 20\n",
      "Page: 21\n",
      "Page: 22\n",
      "Page: 23\n",
      "Page: 24\n",
      "Page: 25\n",
      "Page: 26\n",
      "Page: 27\n",
      "Page: 28\n",
      "Page: 29\n",
      "Page: 30\n",
      "Page: 31\n",
      "Page: 32\n",
      "Page: 33\n",
      "Page: 34\n"
     ]
    }
   ],
   "source": [
    "#Going through each page of the Mother Jones climate change tag to gather articles\n",
    "articles=[]\n",
    "MAX_PAGES = 35\n",
    "\n",
    "for i in range(1, MAX_PAGES):\n",
    "    print(\"Page: {}\".format(i)) #Printing current page being scrapped for articles\n",
    "    links = get_links(\"https://www.motherjones.com/topics/climate-change/page/{}/\".format(i))\n",
    "    \n",
    "    for link in links:\n",
    "        articles.append(get_text(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680\n"
     ]
    }
   ],
   "source": [
    "articles = list(filter(None, articles)) \n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(articles, columns=['article']).drop_duplicates()\n",
    "df['denial?'] = [0]*len(df.index)\n",
    "df.to_csv('mother_jones_articles.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
